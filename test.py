import gc

import gradio as gr
import torch
from gradio import ChatMessage
from transformers import AutoTokenizer, TextIteratorStreamer, pipeline

MODEL = "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
streamer = TextIteratorStreamer(
    tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True
)

pipe = pipeline(
    "text-generation",
    model=MODEL,
    tokenizer=tokenizer,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    max_new_tokens=512,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    streamer=streamer,
    eos_token_id=tokenizer.eos_token_id,
)

NO_INPUT_WARNING = "ì•„ë¬´ê²ƒë„ ì…ë ¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë™í™” ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
CONTINUE_PROMPT = "ì´ì „ ë™í™”ë¥¼ ì´ì–´ì„œ ì¶œë ¥í•©ë‹ˆë‹¤."


def answer(user_input, history, top_p, top_k, temperature):
    if user_input == "":
        if history[-1][1] == NO_INPUT_WARNING:
            yield NO_INPUT_WARNING
        else:
            yield CONTINUE_PROMPT
    else:
        gc.collect()
        torch.cuda.empty_cache()
    history.append(ChatMessage(role="user", content=user_input))
    print(history)

    generate_kwargs = dict(
            history,
            pad_token_id=tokenizer.eos_token_id,
            streamer=streamer,
            max_new_tokens=512,
            do_sample=True,
            top_p=top_p,
            top_k=top_k,
            temperature=temperature,
            num_beams=1,
            stopping_criteria=StoppingCriteriaList([stop]),
        )

        t = Thread(target=model.generate, kwargs=generate_kwargs)
        t.start()
    
    partial_message = ""
    for new_text in streamer:
        partial_message += new_text
        yield partial_message


# Interface
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    chatbot = gr.Chatbot(
        show_copy_button=True,
        avatar_images=("images/user.png", "images/bot.png"),
        render=False,
    )
    textbox = gr.Textbox(
        placeholder="ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”. (ì•„ë¬´ê²ƒë„ ì…ë ¥í•˜ì§€ ì•Šìœ¼ë©´ ì´ì „ ë™í™”ë¥¼ ì´ì–´ì„œ ì¶œë ¥í•©ë‹ˆë‹¤.)",
        scale=7,
        render=False,
    )
    top_p_slider = gr.Slider(0, 1, value=0.95, label="ë‹¨ì–´ ì„ íƒì˜ ë‹¤ì–‘ì„±", render=False)
    top_k_slider = gr.Slider(
        0, 5000, value=2000, label="ë‹¨ì–´ ì„ íƒì˜ ê°€ì§“ìˆ˜", render=False
    )
    temperature_slider = gr.Slider(0, 1, value=1, label="ì°½ì˜ì„±", render=False)
    stop_btn = gr.Button("ë©ˆì¶¤", render=False, variant="stop")

    gr.ChatInterface(
        answer,
        chatbot=chatbot,
        textbox=textbox,
        examples=[
            ["ê¸ˆí™”ë¥¼ ì§€í‚¤ëŠ” ìš©"],
            ["ë§ˆë²• ë™ë¬¼ì›ì˜ ë¹„ë°€"],
            ["ì „ì„¤ì˜ ê²€"],
        ],
        title="TaleGPT (ë™í™” / íŒíƒ€ì§€ ì†Œì„¤ ìƒì„± ì¸ê³µì§€ëŠ¥)",
        description="Â© ì¤‘ì•™ëŒ€í•™êµ ê¸°ê³„í•™ìŠµìë™í™”ì—°êµ¬ì‹¤ - CAU AutoML Lab",
        submit_btn="ì œì¶œ",
        stop_btn="ë©ˆì¶¤",
        retry_btn="ğŸ”„ ë‹¤ì‹œ ì‹œë„",
        undo_btn="â†©ï¸ ë˜ëŒë¦¬ê¸°",
        clear_btn="ğŸ—‘ï¸ ì§€ìš°ê¸°",
        additional_inputs=[top_p_slider, top_k_slider, temperature_slider],
        concurrency_limit=2,
    )

demo.queue().launch(share=True, auth=("automl", "208217"))
