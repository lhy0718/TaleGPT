import os

os.environ['CUDA_VISIBLE_DEVICES'] = '0'

from threading import Thread

import gradio as gr
import torch
from transformers import (AutoModelForCausalLM, AutoTokenizer,
                          StoppingCriteria, StoppingCriteriaList,
                          TextIteratorStreamer, BitsAndBytesConfig)

MODEL = "beomi/KoAlpaca-Polyglot-12.8B"
if torch.cuda.is_available():
    model = AutoModelForCausalLM.from_pretrained(
        MODEL,
        device_map="auto",
        load_in_8bit=True,
        revision="8bit",
        # torch_dtype=torch.float16,
    )
else:
    model = AutoModelForCausalLM.from_pretrained(MODEL)
    
tokenizer = AutoTokenizer.from_pretrained(MODEL)

class StopOnTokens(StoppingCriteria):
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        stop_ids = [2, 6, 31] # "<|endoftext|>": 2, "#": 6, "<": 31
        for stop_id in stop_ids:
            if input_ids[0][-1] == stop_id:
                return True
        return False
    

def convert_history_item_to_message(history_item: list) -> str:
    # return f"### ëª…ë ¹ì–´:\n{history_item[0].strip()}\n### ì¶œë ¥:\n{history_item[1].split('#')[0].strip()}"
    return f"### ì œëª©: {history_item[0].strip()}\n### ì¶œë ¥: {history_item[1].split('#')[0].strip()}"


def answer(message, history, top_p, top_k, temperature):
    with open('prompt.txt') as f:
        prompt = f.read()
    pre_system_message = prompt
    curr_system_message = '''
### ëª…ë ¹ì–´:
ë‹¤ìŒ ì œëª©ì˜ ë™ìš”ë¥¼ ìƒì„±í•˜ì„¸ìš”.
'''

    stop = StopOnTokens()

    message = message or ""
    history_transformer_format = history + [[message, ""]]
    
    messages = (
        pre_system_message
        + "\n".join(map(convert_history_item_to_message, history_transformer_format[:-1])) 
        + curr_system_message 
        + convert_history_item_to_message(history_transformer_format[-1])
    )
    print("\n========== Input Messages")
    print(messages)

    model_inputs = tokenizer([messages], return_token_type_ids=False, return_tensors="pt")
    if torch.cuda.is_available():
        model_inputs = model_inputs.to("cuda")
    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)
    generate_kwargs = dict(
        model_inputs,
        pad_token_id=tokenizer.eos_token_id,
        streamer=streamer,
        max_new_tokens=512,
        do_sample=True,
        top_p=top_p,
        top_k=top_k,
        temperature=temperature,
        num_beams=1,
        stopping_criteria=StoppingCriteriaList([stop])
    )
    t = Thread(target=model.generate, kwargs=generate_kwargs)
    t.start()

    partial_message  = ""
    for new_token in streamer:
        if new_token not in ["<", "#"]:
            partial_message += new_token
            yield partial_message

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    chatbot = gr.Chatbot(
        show_copy_button=True,
        avatar_images=("images/user.png", "images/bot.png"),
        render=False,
    )
    textbox = gr.Textbox(
        placeholder = "ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”.",
        scale=7,
        render=False,
    )
    top_p_slider = gr.Slider(0, 1, value=0.95, label="ë‹¨ì–´ ì„ íƒì˜ ë‹¤ì–‘ì„±", render=False)
    top_k_slider = gr.Slider(0, 5000, value=2000, label="ë‹¨ì–´ ì„ íƒì˜ ê°€ì§“ìˆ˜", render=False)
    temperature_slider = gr.Slider(0, 1, value=1, label="ì°½ì˜ì„±", render=False)
    
    gr.ChatInterface(
        answer,
        chatbot=chatbot,
        textbox=textbox,
        examples=[
            # ["ê¸ˆí™”ë¥¼ ì§€í‚¤ëŠ” ìš©"],
            # ["ë§ˆë²• ë™ë¬¼ì›ì˜ ë¹„ë°€"],
            # ["ì „ì„¤ì˜ ê²€"],
            ["ë‹¬ë‹˜"],
            ["ìš©ì´ ë‚˜íƒ€ë‚¬ë‹¤!"],
            ["ê°€ì¡±ì˜ ì‚¬ë‘"],
        ],
        # title="TaleGPT (ë™í™” / íŒíƒ€ì§€ ì†Œì„¤ ìƒì„± ì¸ê³µì§€ëŠ¥)",
        title="LyricGPT",
        description="Â© ì¤‘ì•™ëŒ€í•™êµ ê¸°ê³„í•™ìŠµìë™í™”ì—°êµ¬ì‹¤ - CAU AutoML Lab",
        submit_btn="ì œì¶œ",
        stop_btn="ë©ˆì¶¤",
        retry_btn="ğŸ”„ ë‹¤ì‹œ ì‹œë„",
        undo_btn="â†©ï¸ ë˜ëŒë¦¬ê¸°",
        clear_btn="ğŸ—‘ï¸ ì§€ìš°ê¸°",
        additional_inputs=[top_p_slider, top_k_slider, temperature_slider]
    )
    
    # with open('createpopup.js') as f:
    #     script = f.read()
    # gr.HTML(f'<script type="text/javascript">{script}</script>')
    # print_button = gr.Button(
    #     value="ğŸ–¨ï¸ í”„ë¦°íŠ¸",
    #     size="sm",
    # )
    # print_button.click(fn=None, _js="createPopupWithText()") # not working

demo.queue().launch(share=True)