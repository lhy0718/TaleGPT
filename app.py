import gc
from threading import Thread

import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer

MODEL = "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForCausalLM.from_pretrained(
    MODEL,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
)

NO_INPUT_WARNING = "ì•„ë¬´ê²ƒë„ ì…ë ¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë™í™” ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
INITIAL_PROMPT = "ì…ë ¥ë˜ëŠ” ì œëª©ì„ ë°”íƒ•ìœ¼ë¡œ ëŒ€í•œ ì•„ë™ì„ íƒ€ê²Ÿìœ¼ë¡œ í•˜ëŠ” ë™í™” ë˜ëŠ” ê°ì¢… íŒíƒ€ì§€ ìš”ì†Œê°€ ë‚œë¬´í•˜ëŠ” íŒíƒ€ì§€ ì†Œì„¤ì„ ì¶œë ¥í•©ë‹ˆë‹¤."
CONTINUE_PROMPT = "ì¶œë ¥ì´ ëë‚œ ë¶€ë¶„ë¶€í„° ë™í™”ë¥¼ ì´ì–´ì„œ ì¶œë ¥í•©ë‹ˆë‹¤."


def convert_history_to_messages(history: list = []) -> list:
    messages = [{"role": "system", "content": INITIAL_PROMPT}]
    for user, assistant in history:
        messages.append({"role": "user", "content": user})
        messages.append({"role": "assistant", "content": assistant})
    return messages


def answer(user_input, history, top_p, top_k, temperature):
    if (len(history) == 0 or history[-1][1] == NO_INPUT_WARNING) and user_input == "":
        yield NO_INPUT_WARNING
    else:
        gc.collect()
        torch.cuda.empty_cache()

        if user_input.strip() == "":  # ì´ì „ ë™í™”ë¥¼ ì´ì–´ì„œ ì¶œë ¥
            history = convert_history_to_messages(history[-4:])
            history.append({"role": "system", "content": CONTINUE_PROMPT})
        else:  # ìƒˆë¡œìš´ ë™í™” ìƒì„±
            history = convert_history_to_messages()
            history.append({"role": "user", "content": user_input})

        input_ids = tokenizer.apply_chat_template(
            history, tokenize=True, add_generation_prompt=True, return_tensors="pt"
        ).to("cuda")

        streamer = TextIteratorStreamer(
            tokenizer, timeout=3, skip_prompt=True, skip_special_tokens=True
        )

        generate_kwargs = dict(
            inputs=input_ids,
            mask_token_id=tokenizer.mask_token_id,
            pad_token_id=tokenizer.eos_token_id,
            streamer=streamer,
            max_new_tokens=512,
            do_sample=True,
            top_p=top_p,
            top_k=top_k,
            temperature=temperature,
            num_beams=1,
        )

        t = Thread(target=model.generate, kwargs=generate_kwargs)
        t.start()

        partial_message = ""
        for new_text in streamer:
            partial_message += new_text
            yield partial_message


# Interface
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    chatbot = gr.Chatbot(
        show_copy_button=True,
        avatar_images=("images/user.png", "images/bot.png"),
        render=False,
    )
    textbox = gr.Textbox(
        placeholder="ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”. (ì•„ë¬´ê²ƒë„ ì…ë ¥í•˜ì§€ ì•Šìœ¼ë©´ ì´ì „ ë™í™”ë¥¼ ì´ì–´ì„œ ì¶œë ¥í•©ë‹ˆë‹¤.)",
        scale=7,
        render=False,
    )
    top_p_slider = gr.Slider(0, 1, value=0.95, label="ë‹¨ì–´ ì„ íƒì˜ ë‹¤ì–‘ì„±", render=False)
    top_k_slider = gr.Slider(
        0, 5000, value=2000, label="ë‹¨ì–´ ì„ íƒì˜ ê°€ì§“ìˆ˜", render=False
    )
    temperature_slider = gr.Slider(0, 1, value=1, label="ì°½ì˜ì„±", render=False)

    gr.ChatInterface(
        answer,
        chatbot=chatbot,
        textbox=textbox,
        examples=[
            ["ê¸ˆí™”ë¥¼ ì§€í‚¤ëŠ” ìš©"],
            ["ë§ˆë²• ë™ë¬¼ì›ì˜ ë¹„ë°€"],
            ["ì „ì„¤ì˜ ê²€"],
        ],
        title="TaleGPT (ë™í™” / íŒíƒ€ì§€ ì†Œì„¤ ìƒì„± ì¸ê³µì§€ëŠ¥)",
        description="Â© ì¤‘ì•™ëŒ€í•™êµ ê¸°ê³„í•™ìŠµìë™í™”ì—°êµ¬ì‹¤ - CAU AutoML Lab",
        submit_btn="ì œì¶œ",
        stop_btn="ë©ˆì¶¤",
        retry_btn="ğŸ”„ ë‹¤ì‹œ ì‹œë„",
        undo_btn="â†©ï¸ ë˜ëŒë¦¬ê¸°",
        clear_btn="ğŸ—‘ï¸ ì§€ìš°ê¸°",
        additional_inputs=[top_p_slider, top_k_slider, temperature_slider],
    )

demo.queue(default_concurrency_limit=2).launch(
    # share=True,
    # auth=("automl", "208217"),
)
