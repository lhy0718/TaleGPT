import os
from tkinter import NO

os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

import gc
import json
from threading import Thread

import gradio as gr
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    StoppingCriteria,
    StoppingCriteriaList,
    TextIteratorStreamer,
)

INITIAL_PROMPT = "### Î™ÖÎ†πÏñ¥:\nÎã§Ïùå Î™ÖÎ†πÏóê ÎåÄÌïú ÏïÑÎèôÏùÑ ÌÉÄÍ≤üÏúºÎ°ú ÌïòÎäî ÎèôÌôî ÎòêÎäî Í∞ÅÏ¢Ö ÌåêÌÉÄÏßÄ ÏöîÏÜåÍ∞Ä ÎÇúÎ¨¥ÌïòÎäî ÌåêÌÉÄÏßÄ ÏÜåÏÑ§ÏùÑ Ï∂úÎ†•Ìï©ÎãàÎã§.\n"
GEN_PROMPT_FORMAT = "### Ï†úÎ™©:\n{title}\n### ÌåêÌÉÄÏßÄ ÎèôÌôî Ï∂úÎ†•:\n{story}"
CONTINUEW_GEN_PROMPT_FORMAT = "### Ïù¥Ï†ÑÍ≥º Ïù¥Ïñ¥ÏÑú ÎèôÌôî Í≥ÑÏÜç Ï∂úÎ†•:\n{story}"
NO_INPUT_WARNING = "ÏïÑÎ¨¥Í≤ÉÎèÑ ÏûÖÎ†•ÌïòÏßÄ ÏïäÏïòÏäµÎãàÎã§. ÎèôÌôî Ï†úÎ™©ÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî."

MODEL = "beomi/KoAlpaca-Polyglot-12.8B"  # Ï†úÏùº Ï¢ãÏùå
# MODEL = "beomi/Solar-Ko-Recovery-11B"
# MODEL = "beomi/OPEN-SOLAR-KO-10.7B"
# MODEL = "beomi/Llama-3-Open-Ko-8B"
# MODEL = "MLP-KTLim/llama-3-Korean-Bllossom-8B"

if torch.cuda.is_available():
    quantization_config = BitsAndBytesConfig(load_in_8bit=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL,
        device_map="auto",
        quantization_config=quantization_config,
    )
else:
    model = AutoModelForCausalLM.from_pretrained(MODEL)

tokenizer = AutoTokenizer.from_pretrained(MODEL)


class StopOnTokens(StoppingCriteria):
    def __call__(
        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs
    ) -> bool:
        stop_ids = [tokenizer.eos_token_id] + tokenizer.convert_tokens_to_ids(
            ["#", "<"]
        )
        for stop_id in stop_ids:
            if input_ids[0][-1] == stop_id:
                return True
        return False


def convert_history_item_to_message(history_item: list) -> str:
    title, story = history_item
    if story == NO_INPUT_WARNING:
        return ""
    title = title or ""
    story = story or ""
    if title == "":
        return CONTINUEW_GEN_PROMPT_FORMAT.format(story=story)
    return GEN_PROMPT_FORMAT.format(title=title, story=story)


def answer(user_input, history, top_p, top_k, temperature):
    if (len(history) == 0 or history[-1][1] == NO_INPUT_WARNING) and user_input == "":
        yield NO_INPUT_WARNING
    else:
        gc.collect()
        torch.cuda.empty_cache()

        with open("fewshot.json") as f:
            fewshot = json.load(f)
            history = fewshot + history

        stop = StopOnTokens()

        history += [[user_input, ""]]  # history: [[title, story], ...]

        input_text = INITIAL_PROMPT + "\n".join(
            [
                convert_history_item_to_message(item) for item in history[-6:]
            ]  # ÏµúÍ∑º 5Í∞úÏùò fewshot + user_input
        )

        print("\n========== Input Messages")
        print(input_text)

        model_inputs = tokenizer(
            [input_text], return_token_type_ids=False, return_tensors="pt"
        )
        if torch.cuda.is_available():
            model_inputs = model_inputs.to("cuda")

        streamer = TextIteratorStreamer(
            tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True
        )

        generate_kwargs = dict(
            model_inputs,
            pad_token_id=tokenizer.eos_token_id,
            streamer=streamer,
            max_new_tokens=512,
            do_sample=True,
            top_p=top_p,
            top_k=top_k,
            temperature=temperature,
            num_beams=1,
            stopping_criteria=StoppingCriteriaList([stop]),
        )

        t = Thread(target=model.generate, kwargs=generate_kwargs)
        t.start()

        partial_message = ""
        for new_token in streamer:
            if new_token not in ["<", "#"]:
                partial_message += new_token
                yield partial_message


# Interface
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    chatbot = gr.Chatbot(
        show_copy_button=True,
        avatar_images=("images/user.png", "images/bot.png"),
        render=False,
    )
    textbox = gr.Textbox(
        placeholder="Ïó¨Í∏∞Ïóê ÏûÖÎ†•ÌïòÏÑ∏Ïöî. (ÏïÑÎ¨¥Í≤ÉÎèÑ ÏûÖÎ†•ÌïòÏßÄ ÏïäÏúºÎ©¥ Ïù¥Ï†Ñ ÎèôÌôîÎ•º Ïù¥Ïñ¥ÏÑú Ï∂úÎ†•Ìï©ÎãàÎã§.)",
        scale=7,
        render=False,
    )
    top_p_slider = gr.Slider(0, 1, value=0.95, label="Îã®Ïñ¥ ÏÑ†ÌÉùÏùò Îã§ÏñëÏÑ±", render=False)
    top_k_slider = gr.Slider(
        0, 5000, value=2000, label="Îã®Ïñ¥ ÏÑ†ÌÉùÏùò Í∞ÄÏßìÏàò", render=False
    )
    temperature_slider = gr.Slider(0, 1, value=1, label="Ï∞ΩÏùòÏÑ±", render=False)

    gr.ChatInterface(
        answer,
        chatbot=chatbot,
        textbox=textbox,
        examples=[
            ["Í∏àÌôîÎ•º ÏßÄÌÇ§Îäî Ïö©"],
            ["ÎßàÎ≤ï ÎèôÎ¨ºÏõêÏùò ÎπÑÎ∞Ä"],
            ["Ï†ÑÏÑ§Ïùò Í≤Ä"],
        ],
        title="TaleGPT (ÎèôÌôî / ÌåêÌÉÄÏßÄ ÏÜåÏÑ§ ÏÉùÏÑ± Ïù∏Í≥µÏßÄÎä•)",
        description="¬© Ï§ëÏïôÎåÄÌïôÍµê Í∏∞Í≥ÑÌïôÏäµÏûêÎèôÌôîÏó∞Íµ¨Ïã§ - CAU AutoML Lab",
        submit_btn="Ï†úÏ∂ú",
        stop_btn="Î©àÏ∂§",
        retry_btn="üîÑ Îã§Ïãú ÏãúÎèÑ",
        undo_btn="‚Ü©Ô∏è ÎêòÎèåÎ¶¨Í∏∞",
        clear_btn="üóëÔ∏è ÏßÄÏö∞Í∏∞",
        additional_inputs=[top_p_slider, top_k_slider, temperature_slider],
    )

demo.queue().launch(share=False, auth=("automl", "208217"), server_name="0.0.0.0")
