import gradio as gr
from huggingface_hub import InferenceClient

NO_INPUT_WARNING = "ì•„ë¬´ê²ƒë„ ì…ë ¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë™í™” ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
INITIAL_PROMPT = "ì…ë ¥ë˜ëŠ” ì œëª©ì„ ë°”íƒ•ìœ¼ë¡œ ëŒ€í•œ ì•„ë™ì„ íƒ€ê²Ÿìœ¼ë¡œ í•˜ëŠ” ë™í™” ë˜ëŠ” ê°ì¢… íŒíƒ€ì§€ ìš”ì†Œê°€ ë‚œë¬´í•˜ëŠ” íŒíƒ€ì§€ ì†Œì„¤ì„ ì¶œë ¥í•©ë‹ˆë‹¤."
CONTINUE_PROMPT = "ì¶œë ¥ì´ ëë‚œ ë¶€ë¶„ë¶€í„° ë™í™”ë¥¼ ì´ì–´ì„œ ì¶œë ¥í•©ë‹ˆë‹¤."

client = InferenceClient(base_url="http://127.0.0.1:8080")


def convert_history_to_messages(history: list = []) -> list:
    messages = [{"role": "system", "content": INITIAL_PROMPT}]
    for user, assistant in history:
        messages.append({"role": "user", "content": user})
        messages.append({"role": "assistant", "content": assistant})
    return messages


def inference(user_input, history, top_p, temperature):
    if (len(history) == 0 or history[-1][1] == NO_INPUT_WARNING) and user_input == "":
        yield NO_INPUT_WARNING
    else:
        if user_input.strip() == "":  # ì´ì „ ë™í™”ë¥¼ ì´ì–´ì„œ ì¶œë ¥
            history = convert_history_to_messages(
                history[-4:]
            )  # ë§ˆì§€ë§‰ 4ê°œì˜ ëŒ€í™”ë§Œ ì´ì–´ì„œ ì¶œë ¥
            history.append({"role": "system", "content": CONTINUE_PROMPT})
        else:  # ìƒˆë¡œìš´ ë™í™” ìƒì„±
            history = convert_history_to_messages()
            history.append({"role": "user", "content": user_input})

        partial_message = ""
        output = client.chat.completions.create(
            messages=history,
            stream=True,
            max_tokens=1024,
            temperature=temperature,
            top_p=top_p,
        )

        for chunk in output:
            partial_message += chunk.choices[0].delta.content
            yield partial_message


# Interface
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    chatbot = gr.Chatbot(
        show_copy_button=True,
        avatar_images=("images/user.png", "images/bot.png"),
        render=False,
    )
    textbox = gr.Textbox(
        placeholder="ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”. (ì•„ë¬´ê²ƒë„ ì…ë ¥í•˜ì§€ ì•Šìœ¼ë©´ ì´ì „ ë™í™”ë¥¼ ì´ì–´ì„œ ì¶œë ¥í•©ë‹ˆë‹¤.)",
        scale=7,
        render=False,
    )
    top_p_slider = gr.Slider(0, 1, value=0.95, label="ë‹¨ì–´ ì„ íƒì˜ ë‹¤ì–‘ì„±", render=False)
    top_k_slider = gr.Slider(
        0, 5000, value=2000, label="ë‹¨ì–´ ì„ íƒì˜ ê°€ì§“ìˆ˜", render=False
    )
    temperature_slider = gr.Slider(0, 1, value=1, label="ì°½ì˜ì„±", render=False)

    gr.ChatInterface(
        inference,
        chatbot=chatbot,
        textbox=textbox,
        examples=[
            ["ê¸ˆí™”ë¥¼ ì§€í‚¤ëŠ” ìš©"],
            ["ë§ˆë²• ë™ë¬¼ì›ì˜ ë¹„ë°€"],
            ["ì „ì„¤ì˜ ê²€"],
        ],
        title="TaleGPT (ë™í™” / íŒíƒ€ì§€ ì†Œì„¤ ìƒì„± ì¸ê³µì§€ëŠ¥)",
        description="Â© ì¤‘ì•™ëŒ€í•™êµ ê¸°ê³„í•™ìŠµìë™í™”ì—°êµ¬ì‹¤ - CAU AutoML Lab",
        submit_btn="ì œì¶œ",
        stop_btn="ë©ˆì¶¤",
        retry_btn="ğŸ”„ ë‹¤ì‹œ ì‹œë„",
        undo_btn="â†©ï¸ ë˜ëŒë¦¬ê¸°",
        clear_btn="ğŸ—‘ï¸ ì§€ìš°ê¸°",
        additional_inputs=[top_p_slider, temperature_slider],
    )

demo.queue(default_concurrency_limit=2).launch(
    share=True,
    # auth=("automl", "208217"),
)
